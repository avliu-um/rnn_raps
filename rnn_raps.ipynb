{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8680a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657ae59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c6610e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './rap_lyrics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a3660f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get verses\n",
    "verses = []\n",
    "current_verse = []\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:  # Non-empty line\n",
    "            current_verse.extend(line.split())\n",
    "        elif current_verse:  # Empty line, but we have words in the current verse\n",
    "            verses.append(current_verse)\n",
    "            current_verse = []\n",
    "\n",
    "    if current_verse:  # If there's a verse left after reading the file\n",
    "        verses.append(current_verse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d54befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the verses here so that train/test is independent of rapper \n",
    "\n",
    "import random\n",
    "random.shuffle(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afd8b39c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7836"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0041f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd281512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize verses (words --> numbers)\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(verses)\n",
    "sequences = tokenizer.texts_to_sequences(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f260844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8c11a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into RNN-style splits \n",
    "# (words 1-50 predict word 51, words 2-51 predict word 52, etc.)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "\n",
    "# Iterate through the sequences of tokens (for us, verses)\n",
    "for seq in sequences:\n",
    "\n",
    "    # Create multiple training examples from each sequence\n",
    "    for i in range(training_length, len(seq)):\n",
    "        \n",
    "        # Extract the features and label\n",
    "        extract = seq[i - training_length:i + 1]\n",
    "\n",
    "        # Set the features and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "        \n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8c3cb09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620442, 50)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e081ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot the labels (i.e. Y)\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "label_array = np.zeros((len(features), num_words), dtype=np.int8)\n",
    "\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_array[example_index, word_index] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a678c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620442, 33434)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99edb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e2497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "n = len(features)\n",
    "train_ratio = 0.8\n",
    "split = int(n*train_ratio)\n",
    "\n",
    "train_x = features[:split]\n",
    "train_y = label_array[:split]\n",
    "\n",
    "test_x = features[split:]\n",
    "test_y = label_array[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "248e939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496353, 33434)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6416b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c56672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d668773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now at a point where we've split the text data into features and labels via Tokenizer \n",
    "# (and one-hotted the labels into \"label_array\")\n",
    "\n",
    "# Might be time to head back to Brownlee's tutorial to figure out how to split data to X/Y\n",
    "# (Before that, stay on the Koehrsen tutorial to get embeddings)\n",
    "\n",
    "# Then, on to RNN building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d867980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps:\n",
    "# (1) Generate embeddings of features (Koehrsen)\n",
    "# (2) Split train/test (Brownlee)\n",
    "# (3) Build RNN model (Koehrsen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4295584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained embeddings\n",
    "\n",
    "# Load in embeddings\n",
    "glove_vectors = './glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "\n",
    "# Extract the vectors and words\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "# Create lookup of words to vectors\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "# New matrix to hold word embeddings\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "feb1a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(tokenizer.word_index.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa57c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bcace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary time\n",
    "\n",
    "# num_words: size of vocabulary\n",
    "# training_length: input feature length in the time direction (i.e. 50 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88d2da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              #input_length = training_length,\n",
    "              output_dim=100,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "156c1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 100)         3343400   \n",
      "                                                                 \n",
      " masking_3 (Masking)         (None, None, 100)         0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                42240     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 33434)             2173210   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,563,010\n",
      "Trainable params: 2,219,610\n",
      "Non-trainable params: 3,343,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf5513",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=20, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42da407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
